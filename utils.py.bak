'''
# utils.py

import re
import nltk
import os

nltk.download("punkt")

def clean_and_tokenize(text):
    '''
    Cleans and tokenizes the text.
    Removes HTML tags, square brackets, parentheses, URLs, non-alphanumeric characters,
    numbers, and converts the text to lowercase.
    '''
    text = re.sub(r'<[^>]*>', '', text)  # remove HTML tags
    text = re.sub(r'\[.*?\]', '', text)  # remove square brackets
    text = re.sub(r'\(.*?\)', '', text)  # remove parentheses
    text = re.sub(r'\b(?:http|ftp)s?://\S+', '', text)  # remove URLs
    text = re.sub(r'\W', ' ', text)  # remove non-alphanumeric characters
    text = re.sub(r'\d+', '', text)  # remove numbers
    text = text.lower()
    
    return nltk.word_tokenize(text)

def format_documents(documents):
    '''
    Formats the list of documents.
    Returns a numbered list with the basename of the source file and the page content of each document.
    '''
    numbered_docs = "\n".join([f"{i+1}. {os.path.basename(doc.metadata['source'])}: {doc.page_content}" for i, doc in enumerate(documents)])
    return numbered_docs

def format_user_question(question):
    '''
    Formats the user's question.
    Removes excessive whitespace.
    '''
    question = re.sub(r'\s+', ' ', question).strip()
    return question
'''

In the refactored code:
- The code has been organized into functions with a single responsibility and clear purpose.
- Descriptive variable and function names have been used.
- Comments and docstrings have been added to explain complex logic.
- The code has been formatted according to PEP 8 guidelines.
- Redundant import statement for the 'nltk' library has been removed.
- Error handling and exception management are not needed as the functions do not require it.
- No performance bottlenecks were identified.
- No dead or unused code was found.